{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-permit",
   "metadata": {},
   "source": [
    "<h2> This notebook achieves several things.    </h2>\n",
    "\n",
    "1. It combines the Bonsai output of 470nm and 415nm traces into one side-by-side sheet\n",
    "2. It crops to a start and end time (primarily to remove start/end artifacts due to plug/unplug)\n",
    "3. It fits the 470 to the 415 curve, plots these\n",
    "4. It subtracts fitted 470 from fitted 415 for the deltaF\n",
    "5. It has the user define a baseline period (start and end in seconds)\n",
    "6. It calculates the median of this baseline and subtracts this from the entire trace\n",
    "7. It calculates a % change in dF/F0\n",
    "8. It calculates a Z-score\n",
    "9. it combines the behaviour sheet and the camera sheet into one\n",
    "10. Together this produces 3 output files (ID_m19_SignalZscore; ID_SignalPercentDelta; ID_BehaviourTimeStamped), which are used as inputs for DLC heatmapping, or perievent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import array as arr\n",
    "import scipy\n",
    "from scipy import signal as ss\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.linear_model import HuberRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-blank",
   "metadata": {},
   "source": [
    "## All inputs for the entire sheet can be put in here, and then you should be able to run the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input start time and duration of TOTAL recording in seconds, for cropping\n",
    "StartSecs = 180 ## if using this for peri-event, keep at 0. it will cut the file to this point. \n",
    "DUR = 3600 ## in seconds. make it at least your session, can be longer\n",
    "Hz = 20 ## Capture rate on NPM system for the 470nm channel\n",
    "\n",
    "#INPUT YOUR FOLDER HERE with the 5 files (470, 410, cameracsv, DLCtracking, behaviour)\n",
    "path = r'C:\\Users\\user\\Documents\\temp\\demo' # use your path\n",
    "\n",
    "# DEFINE your baseline period, start + duration\n",
    "StartofBaseline = 5155  # eventually i will get this to pull keydown information from bonsai\n",
    "BaselineDur = 15 ## input duration in minutes\n",
    "BaselineDurFrame = BaselineDur*Hz*60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-instrumentation",
   "metadata": {},
   "source": [
    "# 1) Combine the Bonsai output of 470nm and 415nm traces into one side-by-side sheet, crop to start/end, and plot the traces with a raw delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change second / duration input to frame number\n",
    "StartFrame = int((StartSecs*Hz)+1)\n",
    "EndFrame = DUR*Hz + StartFrame\n",
    "print(\"StartFrame is: \",StartFrame)\n",
    "print(\"EndFrame is: \",EndFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETS 470nm and 415nm in separate dataframes\n",
    "\n",
    "#Makes a new folder to save files in to\n",
    "savepath = os.path.join(path,\"Pre-processing\")\n",
    "if not os.path.exists(savepath):\n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "#finds the 470 and 415 files in the oriignal folder\n",
    "df470path = glob.glob(path + \"/*470*.csv\")\n",
    "print(\"470 file is: \", df470path)\n",
    "df415path = glob.glob(path + \"/*415*.csv\")\n",
    "print(\"415 file is: \", df415path)\n",
    "\n",
    "## Extracts animalID from filename of df470path\n",
    "str = ''.join(df470path)\n",
    "AnimalID = str.split('\\\\')[-1].split('_')[0]\n",
    "\n",
    "\n",
    "## Gets info from 470nm sheet and crops to start time\n",
    "cols470 = ['FrameCounter', 'Timestamp', 'Region0G']\n",
    "csv470 = pd.read_csv(df470path[0], skiprows = range(1,StartFrame), usecols=cols470, )\n",
    "# puts that into a dataframe\n",
    "df470=pd.DataFrame(csv470.values, columns = [\"Frame\", \"Timestamp\", \"470nm\"])\n",
    "#crops to end frame\n",
    "df470 = df470[:EndFrame] \n",
    "#df470\n",
    "\n",
    "## Gets only the 415nm trace from 415nm sheet\n",
    "cols415 = ['Region0G']\n",
    "csv415 = pd.read_csv(df415path[0], skiprows = range(1,StartFrame), usecols=cols415)\n",
    "# puts that into a dataframe\n",
    "df415=pd.DataFrame(csv415.values, columns = [\"415nm\"])\n",
    "#crops to end frame\n",
    "df415 = df415[:EndFrame] \n",
    "#df415\n",
    "\n",
    "# Combines the two dataframes so 470/415 are side-by-side\n",
    "combinedone=pd.concat([df470,df415], axis=1)\n",
    "#combinedone\n",
    "\n",
    "# Plot and save the trace you've segmented for delta, 470 and 415. For Delta only, use below function\n",
    "\n",
    "x = combinedone['Timestamp']\n",
    "y1 = combinedone['470nm']\n",
    "y2 = combinedone['415nm']\n",
    "\n",
    "plt.plot(x, y1, color='green', linewidth=0.2, label = \"470\")\n",
    "plt.plot(x, y2, color='black', linewidth=0.2, label = \"415\")\n",
    "plt.legend(['470', '415'], loc='best')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Activity(AU)')\n",
    "\n",
    "## Save the above plot, input what you want to call it\n",
    "#filename = input(\"Save as what? \")\n",
    "#current_path = os.getcwd()\n",
    "## Save the above plot\n",
    "plt.savefig(savepath+'/'+AnimalID + \"_\" + \"_RawTraces.svg\", dpi=600, orientation='landscape')\n",
    "\n",
    "print('Success! Saved as {}'.format(savepath+'/'+AnimalID + \"_RawTraces.svg\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Write combinedone df to csv, using animal ID\n",
    "#combinedone.to_csv(savepath+'/'+AnimalID + \"_RawTraces.csv\", encoding='utf-8')\n",
    "#print('Success! Saved as {}'.format(savepath+'/'+AnimalID + \"_RawTraces.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-steal",
   "metadata": {},
   "source": [
    "# 2) Fit 470 to 415 curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For now, if they are uneven # of frames, you'll need to manually remove the last frame from the one with the highest\n",
    "#### Adjust this to take these values from the newly created m19_rawtraces.csv\n",
    "raw_signal = df470['470nm']\n",
    "raw_reference = df415['415nm']\n",
    "\n",
    "print(\"Signal, number of frames: \",raw_signal.shape)\n",
    "print(\"Control, number of frames: \",raw_reference.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting and comparing signal and reference, no manipulations occur here, just plots the same data from the above figure, but on different scales\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(raw_signal,'blue',linewidth=1.5)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(raw_reference,'purple',linewidth=1.5)\n",
    "ax1.set_title(\"Signal\")\n",
    "ax2.set_title(\"Reference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "##a linear regression of reference and signal\n",
    "##we use the huberregressor as it is robust to large and infrequent outliers, and often not different to Linear Regression.\n",
    "\n",
    "model = HuberRegressor(epsilon=1)\n",
    "n=len(raw_reference)\n",
    "model.fit(raw_reference.values.reshape(n, 1), raw_signal.values.reshape(n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "##we chart the relationship between reference and signal and show the linear fit. This is because any highly correlated activity is likely noise due to fibre bending, etc.\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(raw_reference,raw_signal, 'b.')\n",
    "\n",
    "plt.xlabel(\"isosbestic / 415nm\")\n",
    "plt.ylabel(\"470nm\")\n",
    "arr = model.predict(raw_reference.values.reshape(n, 1))\n",
    "ax1.plot(raw_reference,arr , 'r--',linewidth=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the aligned control (arr) is the (control + y_intercept) * gradient of the control regressed to the signal\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(raw_signal)\n",
    "##yellow is signal aligned noise\n",
    "ax1.plot(arr,'yellow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now that we have the aligned control, we subtract it from the signal\n",
    "res = np.subtract(raw_signal, arr)\n",
    "##and then divide the signal by the control\n",
    "norm_data = np.divide(res, arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_title(\"Cleaned Signal\")\n",
    "ax1.plot(norm_data, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Plot and save the trace you've segmented for delta, 470 and 415. For Delta only, use below function\n",
    "\n",
    "x = combinedone['Timestamp']\n",
    "y1 = raw_signal\n",
    "y2 = raw_reference\n",
    "y3 = norm_data\n",
    "\n",
    "\n",
    "plt.plot(x, y1, color='purple', linewidth=0.5, label = \"Signal\")\n",
    "plt.plot(x, y2, color='red', linewidth=0.5, label = \"Control\")\n",
    "plt.plot(x, y3, color='green', linewidth=0.5, label = \"delta\")\n",
    "plt.legend(['Signal', 'Control', 'delta'], loc='best')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Activity(AU)')\n",
    "\n",
    "## Save the above plot, input what you want to call it\n",
    "#filename = input(\"Save as what? \")\n",
    "#current_path = os.getcwd()\n",
    "## Save the above plot\n",
    "plt.savefig(savepath+'/'+AnimalID + \"_\" + \"_NormalisedTrace.svg\", dpi=600, orientation='landscape')\n",
    "\n",
    "print('Success! Saved as {}'.format(savepath+'/'+AnimalID + \"_NormalisedTrace.svg\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Saves normalised data into a sheet with the raw data too\n",
    "norm_data=pd.DataFrame(norm_data.values, columns = [\"norm_data\"])\n",
    "\n",
    "# Combines the dataframes so raw and normalised data are together\n",
    "combinedtwo=pd.concat([combinedone,norm_data], axis=1)\n",
    "\n",
    "\n",
    "# Write combinedtwo df to csv, using animal ID\n",
    "#combinedtwo.to_csv(savepath+'/'+AnimalID + \"_NormalisedTraces.csv\", encoding='utf-8')\n",
    "#print('Success! Saved as {}'.format(savepath+'/'+AnimalID + \"_NormalisedTraces.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-compilation",
   "metadata": {},
   "source": [
    "# 3) Define Baseline period, removes it from dF to get dF-f0, to be used for %dF-F0/F0 and z-scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in step 1 you have defined the start point, this takes that into consideration\n",
    "BaselineStartFrame = StartofBaseline - StartFrame\n",
    "#print(BaselineStartFrame)\n",
    "BaselineFinalFrame = BaselineDurFrame + BaselineStartFrame\n",
    "\n",
    "## Defines Fzero\n",
    "dfBase = pd.DataFrame(norm_data)\n",
    "dfBase = dfBase[BaselineStartFrame:BaselineFinalFrame] \n",
    "#print(dfBase)\n",
    "#Fzero or F0 is the median of the defined baseline period. this is for subtracting from each dF value\n",
    "Fzero = dfBase.median()\n",
    "print(\"Baseline Fluorescence [Fzero] is: \", Fzero)\n",
    "\n",
    "# Calculates dF-Fzero\n",
    "dfminusbaseline = norm_data - Fzero\n",
    "\n",
    "dfminusbaseline=pd.DataFrame(dfminusbaseline.values, columns = [\"dF-Fzero\"])\n",
    "#print(\"df minus baseline is\", dfminusbaseline)\n",
    "\n",
    "# Combines the two dataframes so 470/415 are side-by-side\n",
    "combinedthree=pd.concat([combinedtwo, dfminusbaseline], axis=1)\n",
    "#combinedthree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-berkeley",
   "metadata": {},
   "source": [
    "# 4) Uses dF-Fzero to calculate %dF  as (dF-F0/F0) and z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fzero is defined above, as is dfminusebaseline\n",
    "pctdFF = np.divide(dfminusbaseline, Fzero)\n",
    "pctdFF = pctdFF.rename(columns={'dF-Fzero': 'pctdFF'})\n",
    "pctdFF\n",
    "\n",
    "# Plot and save the trace you've segmented for delta, 470 and 415. For Delta only, use below function\n",
    "\n",
    "x = combinedthree['Timestamp']\n",
    "y1 = pctdFF\n",
    "\n",
    "\n",
    "plt.plot(x, y1, color='purple', linewidth=0.5, label = \"%dF\")\n",
    "plt.legend([AnimalID+' %dF'], loc='best')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('%dF')\n",
    "\n",
    "\n",
    "## Save the above plot\n",
    "plt.savefig(savepath+'/'+AnimalID + \"_\" + \"_pctdF.svg\", dpi=600, orientation='landscape')\n",
    "\n",
    "print('Percent dFF image saved as {}'.format(savepath+'/'+AnimalID + \"_pctdF.svg\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### calculates z-score\n",
    "\n",
    "ZdF = scipy.stats.zscore(dfminusbaseline, axis=0, ddof=0, nan_policy='propagate')\n",
    "ZdF = pd.DataFrame({\"signal\":ZdF.reshape(len(ZdF))}, index=np.arange(len(ZdF)))\n",
    "#ZdF = ZdF.rename(columns={'dF-Fzero': 'signal'})\n",
    "# Plot and save the trace you've segmented for delta, 470 and 415. For Delta only, use below function\n",
    "\n",
    "x = combinedthree['Timestamp']\n",
    "y1 = ZdF\n",
    "\n",
    "\n",
    "plt.plot(x, y1, color='brown', linewidth=0.5, label = \"Z-score\")\n",
    "plt.legend([AnimalID+' Z-score'], loc='best')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Z-score')\n",
    "\n",
    "## Save the above plot, input what you want to call it\n",
    "#filename = input(\"Save as what? \")\n",
    "#current_path = os.getcwd()\n",
    "## Save the above plot\n",
    "plt.savefig(savepath+'/'+AnimalID + \"_\" + \"_Z-score.svg\", dpi=600, orientation='landscape')\n",
    "\n",
    "print('Z-score image saved as {}'.format(savepath+'/'+AnimalID + \"_Z-score.svg\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## SAVES FINAL OUTPUT CSV to be used with other tools\n",
    "# f = final\n",
    "\n",
    "fTimestamp = combinedthree['Timestamp']\n",
    "fFrame = combinedthree['Frame']\n",
    "f470 = combinedthree['470nm']\n",
    "f415 = combinedthree['415nm']\n",
    "\n",
    "# Write FinalOutput to csv, using animal ID\n",
    "#FullOutput=pd.concat([fFrame, fTimestamp, f470, f415, pctdFF, ZdF], axis=1)\n",
    "#FullOutput.to_csv(savepath+'/'+AnimalID + \"_FullOutput.csv\", encoding='utf-8', index=False)\n",
    "#print('Success! Saved as {}'.format(savepath+'/'+AnimalID + \"_FullOutput.csv\"))\n",
    "\n",
    "# Write zscore, timestamp, frame to csv for peri-event\n",
    "#ZdF = ZdF.rename(columns={'Z-score': 'signal'})\n",
    "SignalZscore=pd.concat([fFrame, fTimestamp, ZdF], axis=1)\n",
    "SignalZscore.to_csv(savepath+'/'+AnimalID + \"_SignalZscore.csv\", encoding='utf-8', index=False)\n",
    "print('Z-score signal saved as {}'.format(savepath+'/'+AnimalID + \"_SignalZscore.csv\"))\n",
    "\n",
    "# Write zscore, timestamp, frame to csv for peri-event\n",
    "pctdFF = pctdFF.rename(columns={'pctdFF': 'signal'})\n",
    "SignalPercentDelta=pd.concat([fFrame, fTimestamp, pctdFF], axis=1)\n",
    "SignalPercentDelta.to_csv(savepath+'/'+AnimalID + \"_SignalPercentDelta.csv\", encoding='utf-8', index=False)\n",
    "print('Percent dFF signal saved as {}'.format(savepath+'/'+AnimalID + \"_SignalPercentDelta.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-container",
   "metadata": {},
   "source": [
    "# 5) This combines your behaviour and cameracsv into one sheet for the peri-event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETS 470nm and 415nm in separate dataframes\n",
    "\n",
    "behaviourpath = glob.glob(path + \"/*behaviour*.csv\")\n",
    "print(\"Behaviour path: \", behaviourpath)\n",
    "camerapath = glob.glob(path + \"/*csvforvideo*.csv\")\n",
    "print(\"Camera path: \", camerapath)\n",
    "\n",
    "## Gets info from behaviour sheet, and camera sheet, and collates them\n",
    "behaviourcols = ['frame', 'behaviour', 'hits']\n",
    "behaviourcsv = pd.read_csv(behaviourpath[0], usecols=behaviourcols)\n",
    "\n",
    "# puts that into a dataframe\n",
    "dfbehaviour=pd.DataFrame(behaviourcsv.values, columns = [\"frame\", \"duration\", \"hits\"])\n",
    "\n",
    "# Gets info from behaviour sheet, and camera sheet, and collates them\n",
    "cameratimestamps = pd.read_csv(camerapath[0], header=None, usecols=[0])\n",
    "\n",
    "# puts that into a dataframe\n",
    "dfcameratimestamps=pd.DataFrame(cameratimestamps.values, columns = [\"Timestamp\"])\n",
    "\n",
    "# Combines the two dataframes so behaviour frames and camera timestmaps are side-by-side\n",
    "behaviourtimestamped=pd.concat([dfbehaviour,dfcameratimestamps], axis=1)\n",
    "\n",
    "# Write combinedone df to csv, using animal ID\n",
    "behaviourtimestamped.to_csv(savepath+'/'+AnimalID + \"_BehaviourTimeStamped.csv\", encoding='utf-8', index=False)\n",
    "print('Files combined as {}'.format(savepath+'/'+AnimalID + \"_BehaviourTimeStamped.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-morrison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
